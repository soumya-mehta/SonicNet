{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4571ef",
   "metadata": {},
   "source": [
    "# SonicNet: Deep Learning for Dynamic Sound Classification using CNN\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0f3c8",
   "metadata": {},
   "source": [
    "### Step 1: Extracting Sound Features with Mel-Frequency Cepstral Coefficients (MFCC) for Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180a909",
   "metadata": {},
   "source": [
    "Each signal possesses unique traits. In sound processing, the mel-frequency cepstrum (MFC) captures the short-term power spectrum of a sound. Mel-frequency cepstral coefficients (MFCCs) represent this spectrum collectively.\n",
    "\n",
    "Through the librosa library, we'll gather the distinct attributes of each audio signal in our dataset, storing them within a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe39b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os, fnmatch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of how librosa handles sound signals, by reading an example audio signal using librosa\n",
    "audio_file_path='UrbanSound8K/17973-2-0-32.wav'\n",
    "\n",
    "librosa_audio_data, librosa_sample_rate = librosa.load(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librosa converts any stereo(2 channel) signal into mono(single channel) \n",
    "# so converted signal is one dimensional since the signals(2 channels) are converted into single channel(mono) \n",
    "\n",
    "print(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_audio_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecee3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the librosa audio data\n",
    "# Audio with 1 channel \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(librosa_audio_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d5b9f",
   "metadata": {},
   "source": [
    "#### Extracting features from all sound signals within the dataset.\n",
    "\n",
    "Now we will calculate the Mel-Frequency Cepstral Coefficients(MFCC) of the audio samples. The MFCC calculate the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. Using this audio signal characteristics we can identify audio features for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=45)   #n_mfcc: number of MFCCs to return \n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting MFC coefficients from signals using librosa\n",
    "\n",
    "def features_extractor(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=45)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all the files in directory\n",
    "def find_files(directory, pattern):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for basename in files:\n",
    "            if fnmatch.fnmatch(basename, pattern):\n",
    "                filename = os.path.join(root, basename)\n",
    "                yield filename\n",
    "\n",
    "\n",
    "dataset =[]                \n",
    "                \n",
    "for filename in find_files(\"UrbanSound8K/audio\", \"*.wav\"):    \n",
    "#    print(\"Found wav source:\", filename)\n",
    "    label = filename.split(\".wav\")[0][-5]\n",
    "    if label == '-':\n",
    "        label = filename.split(\".wav\")[0][-6]\n",
    "    dataset.append({\"file_name\" : filename, \"label\" : label})\n",
    "  \n",
    "    \n",
    "    \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7692c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating every sound file and extract features using MFC Coefficients of librosa \n",
    "# using features_extractor method defined above\n",
    "extracted_features=[]\n",
    "\n",
    "dataset['data'] = dataset['file_name'].apply(features_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1885a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing column names\n",
    "dataset = dataset.rename(columns={'label': 'class'})\n",
    "dataset = dataset.rename(columns={'data': 'feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d646b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa589e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unnecessary column from dataframe\n",
    "dataset.drop(['file_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c91ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting extracted_features to Pandas dataframe\n",
    "extracted_features_df = pd.DataFrame(dataset,columns=['class','feature'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f589bdd",
   "metadata": {},
   "source": [
    "### Defining Train and Validation Test Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f07897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd605ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b77c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing Label Encoding since we need one hot encoded values for output classes in our model (1s and 0s)\n",
    "\n",
    "# logic of one-hot encoding:\n",
    "# 1 0 0 0 0 0 0 0 0 0 => air_conditioner\n",
    "# 0 1 0 0 0 0 0 0 0 0 => car_horn\n",
    "# 0 0 1 0 0 0 0 0 0 0 => children_playing\n",
    "# 0 0 0 1 0 0 0 0 0 0 => dog_bark\n",
    "# ...\n",
    "# 0 0 0 0 0 0 0 0 0 1 => street_music\n",
    "\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e888a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5af694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset as Train and Test\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb306c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08922fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e24947",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f90202",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041361f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4be45",
   "metadata": {},
   "source": [
    "### Step 2: Building and Training a CNN Model with Processed Sound Signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6121a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building our CNN model\n",
    "\n",
    "model=Sequential()\n",
    "# 1. hidden layer\n",
    "model.add(Dense(125,input_shape=(45,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 2. hidden layer\n",
    "model.add(Dense(250))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 3. hidden layer\n",
    "model.add(Dense(125))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f68f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "epochscount = 300\n",
    "num_batch_size = 32\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=epochscount, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f621887",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_test_set_accuracy = model.evaluate(X_test,y_test,verbose=0)\n",
    "print(validation_test_set_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1834186",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c3225",
   "metadata": {},
   "source": [
    "### Step 3: Finally We Predict an Audio File's Class Using Our CNN Deep Learning Model\n",
    "\n",
    "We preprocess the incoming audio data before making class predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfe778",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"UrbanSound8K/example_sound1_children_playing.wav\"\n",
    "sound_signal, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=sound_signal, sr=sample_rate, n_mfcc=45)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09074f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_scaled_features = mfccs_scaled_features.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_scaled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5df793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ea1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66607a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = model.predict(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_classes = [\"air_conditioner\",\"car_horn\",\"children_playing\",\"dog_bark\",\"drilling\", \"engine_idling\", \"gun_shot\", \"jackhammer\", \"siren\", \"street_music\"]\n",
    "\n",
    "result = np.argmax(result_array[0])\n",
    "print(result_classes[result]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
